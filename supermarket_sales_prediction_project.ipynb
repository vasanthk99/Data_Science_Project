{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMYJRojeKZwLL931vQeymkP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vasanthk99/Data_Science_Project/blob/main/supermarket_sales_prediction_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHI353gbCOQM"
      },
      "outputs": [],
      "source": [
        "import pandas, numpy\n",
        "import os\n",
        "from IPython.display import display, HTML\n",
        "import datetime, calendar\n",
        "from plotly import express\n",
        "import plotly.graph_objects as go\n",
        "import matplotlib.pyplot as slspl, seaborn as sestk\n",
        "from sklearn import metrics, decomposition, model_selection, pipeline,preprocessing, feature_selection, linear_model\n",
        "from statsmodels.tsa.arima.model import ARIMA"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SlsDf=pandas.read_csv(\"/content/Amazon Sale Report.csv\")\n",
        "SlsDf.head()"
      ],
      "metadata": {
        "id": "5hW4wfZxTIzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ClnCov(sls):\n",
        "    print(\"==========================================\")\n",
        "    print(\"      Data Infomation\")\n",
        "    print(\"==========================================\")\n",
        "    print(sls.info())\n",
        "    print(\"===================================================\")\n",
        "    print(\"Missing Values Before Cleaning\")\n",
        "    print(\"===================================================\")\n",
        "    print(sls.isnull().sum())\n",
        "    smcov=sum(sls.isnull().sum())\n",
        "    if smcov>0:\n",
        "        covob=sls.dtypes[sls.dtypes=='object'].index.tolist()\n",
        "        covnm=sls.dtypes[sls.dtypes!='object'].index.tolist()\n",
        "        if sum(sls.isna().sum())!=0:\n",
        "            for x in covob:\n",
        "                sls[x]=sls[x].fillna(sls[x].mode()[0])\n",
        "            for y in covnm:\n",
        "                sls[y]=sls[y].fillna(sls[y].mean())\n",
        "        print(\"===================================================\")\n",
        "        print(\"      Missing Values (After Cleaning)\")\n",
        "        print(\"===================================================\")\n",
        "        print(sls.isnull().sum())\n",
        "    else:\n",
        "        print(\"===================================================\")\n",
        "        print(\"      Data has no Missing Values \")\n",
        "        print(\"===================================================\")\n",
        "    return sls\n",
        "SlsDfMS=ClnCov(SlsDf)"
      ],
      "metadata": {
        "id": "xeKhMW1uTqYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CovDates(sls):\n",
        "    sldf=\"Date\"\n",
        "    slmn=[]\n",
        "    for x in range(1,13):\n",
        "        slmn.append(calendar.month_name[x])\n",
        "    dts_sls=sls[sldf].tolist()\n",
        "    slmn,slmn1=[],[]\n",
        "    dates_sls=[]\n",
        "    year_sls=[]\n",
        "    idx_months=[]\n",
        "    new_features=['Year','Days','Month(Num)','Months']\n",
        "    for d in dts_sls:\n",
        "        spl=d.split(\"-\")\n",
        "        year_sls.append(int(\"20\"+spl[2]))\n",
        "        slmn.append(int(spl[0]))\n",
        "        slmn1.append(calendar.month_name[int(spl[0])])\n",
        "        dates_sls.append(int(spl[1]))\n",
        "    for m in slmn:\n",
        "        idx_months.append(slmn[m-1])\n",
        "    try:\n",
        "        sls.insert(1,new_features[0],numpy.array(year_sls,str))\n",
        "        sls.insert(2,new_features[1],dates_sls)\n",
        "        sls.insert(3,new_features[2],idx_months)\n",
        "        sls.insert(4,new_features[3],slmn1)\n",
        "    except:\n",
        "        pass\n",
        "    #sls=sls.drop('Date',axis=1)\n",
        "    return sls\n",
        "SlsDfNew=CovDates(SlsDfMS)\n",
        "SlsDfNew.head()"
      ],
      "metadata": {
        "id": "54NUvaZdT23W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EDA\n"
      ],
      "metadata": {
        "id": "w6iSFliWUh0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SlsDfNew['Date'] = pandas.to_datetime(SlsDfNew['Date'])\n",
        "daily_sales = SlsDfNew.groupby('Date').agg(total_qty=('Qty', 'sum'), total_amount=('Amount', 'sum')).reset_index()\n",
        "slspl.figure(figsize=(8, 3))\n",
        "sestk.lineplot(data=daily_sales, x='Date', y='total_qty', label='Total Quantity Sold', color='orange', linewidth=2)\n",
        "slspl.title('Sales Trends Over Time')\n",
        "slspl.xlabel('Date')\n",
        "slspl.ylabel('Sales')\n",
        "slspl.xticks(rotation=45)\n",
        "slspl.legend()\n",
        "slspl.grid(True)\n",
        "slspl.show()"
      ],
      "metadata": {
        "id": "UWaJVdy4Ubnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RangeIntQ = SlsDfNew['Amount'].quantile(0.75) - SlsDfNew['Amount'].quantile(0.25)\n",
        "UB = SlsDfNew['Amount'].quantile(0.75) + 1.5*RangeIntQ\n",
        "LB = SlsDfNew['Amount'].quantile(0.25) - 1.5*RangeIntQ\n",
        "\n",
        "ValMdn = SlsDfNew['Amount'].median()\n",
        "SlsDfNew['Amount'] = SlsDfNew['Amount'].apply(lambda x: ValMdn if x > UB  or x < LB else x)\n",
        "SlsDfNew['Amount'] = SlsDfNew['Amount'].apply(lambda x: ValMdn if x == 0 else x)\n",
        "\n",
        "fig, my_ax = slspl.subplots(figsize=(7,3))\n",
        "\n",
        "sestk.histplot(data = SlsDfNew['Amount'], ax=my_ax, binwidth=50, kde=True)\n",
        "slspl.grid(linestyle='--',color='b')\n",
        "slspl.title(\"Distribution of Sales Amount across Count of Orders\")\n",
        "slspl.tight_layout()"
      ],
      "metadata": {
        "id": "tw-zxR6zUbzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fetssls=['Status','Fulfilment','ship-service-level','Category','Size']\n",
        "for f in range(len(fetssls)):\n",
        "    SlsTops=SlsDfNew.groupby(fetssls[f])['Amount'].sum().to_frame().sort_values(by=['Amount'],ascending=False).head(10)\n",
        "    SlsTops.plot(kind='bar',color='#800080')\n",
        "    slspl.title('Sales Revenue Amount by {}'.format(fetssls[f]), fontsize=16, weight='bold')\n",
        "    slspl.ylabel('Total Amount (in millions)', fontsize=14)\n",
        "    slspl.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
        "    slspl.xticks(rotation=45, ha=\"right\")\n",
        "    slspl.legend()\n",
        "    slspl.tight_layout()\n",
        "    slspl.show()\n",
        "    display(HTML(SlsTops.to_html()))"
      ],
      "metadata": {
        "id": "RB9G7hihUvXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FEATURE ENCODING"
      ],
      "metadata": {
        "id": "CGE4JvWCU9lG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def DataEnc(sls):\n",
        "    sls1=sls.copy()\n",
        "    print(\"Feature Types Before Encoding\")\n",
        "    print(sls1.info())\n",
        "    obsls=sls1.dtypes[sls1.dtypes=='object'].index.tolist()\n",
        "    numsls=sls1.dtypes[sls1.dtypes!='object'].index.tolist()\n",
        "    sls_objdf=sls1[obsls]\n",
        "    sls_numdf=sls1[numsls]\n",
        "    cols = sls_objdf.columns.tolist()\n",
        "    sls_objdf[cols] = sls_objdf[cols].apply(preprocessing.LabelEncoder().fit_transform)\n",
        "    sls1=pandas.concat([sls_numdf,sls_objdf],axis=1)\n",
        "    if \"Unnamed: 22\" in sls1.columns:\n",
        "        sls1=sls1.drop(\"Unnamed: 22\",axis=1)\n",
        "    print(\"Feature Types After Encoding\")\n",
        "    print(sls1.info())\n",
        "    return sls1\n",
        "SlsDfNew_encoded=DataEnc(SlsDfNew)\n",
        "SlsDfNew_encoded.head()"
      ],
      "metadata": {
        "id": "jmp3RSsaVAi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OUTLIER DETECTION"
      ],
      "metadata": {
        "id": "OM07U30XVU5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def OutVarSales(sls,v1):\n",
        "    arrsls=numpy.array(sls.iloc[:,:-1])\n",
        "    slspca = decomposition.PCA(n_components=2)\n",
        "    slspca.fit(arrsls)\n",
        "    explval=slspca.explained_variance_ratio_\n",
        "    valm=max(explval.tolist())\n",
        "    print(explval)\n",
        "    cmp_slspca=[\"Comp-{}\".format(i+1) for i in range(len(explval.tolist()))]\n",
        "    slspl.figure(figsize=(5,3))\n",
        "    valml=\"%.3f\" % valm\n",
        "    slspl.title(\"PCA Variance (Max: {})\".format(max(explval.tolist())),fontsize=16)\n",
        "    slspl.bar(cmp_slspca,explval.tolist(),width=0.5,color=v1)\n",
        "    slspl.plot(explval.tolist(),\"--yD\")\n",
        "    slspl.xlabel(\"Components\",fontsize=14)\n",
        "    slspl.ylabel(\"PCA Variance\",fontsize=14)\n",
        "    slspl.grid()\n",
        "    slspl.show()\n",
        "    return explval\n",
        "def DataSalScl(sls):\n",
        "    SCLMM = preprocessing.MinMaxScaler()\n",
        "    slsscl=SCLMM.fit_transform(sls)\n",
        "    return slsscl"
      ],
      "metadata": {
        "id": "n4tO7khpVEc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vramnt=[]\n",
        "vramnt.append(OutVarSales(SlsDfNew_encoded.drop(['Date','Amount'],axis=1),\"r\"))\n",
        "\n",
        "FlagVar=[]\n",
        "for vr in vramnt:\n",
        "    for v in vr:\n",
        "        if v>0.8:\n",
        "            FlagVar.append(True)\n",
        "if len(FlagVar)==1 and True in FlagVar:\n",
        "    NrmSls=DataSalScl(SlsDfNew_encoded.drop(['Date','Amount'],axis=1))\n",
        "NrmSls=pandas.DataFrame(NrmSls,columns=SlsDfNew_encoded.drop(['Date','Amount'],axis=1).columns.tolist())\n",
        "NrmSls['Amount']=SlsDfNew_encoded['Amount'].tolist()\n",
        "OutVarSales(NrmSls.drop('Amount',axis=1),\"c\")\n",
        "NrmSls.head(10)"
      ],
      "metadata": {
        "id": "8q-2ZJ1uVaY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FEATURE SELECTION"
      ],
      "metadata": {
        "id": "NAH0xXXKVte8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Func1(sls,nmst):\n",
        "    secl=\"plasma\"\n",
        "    plst=len(sls)\n",
        "    ttl_text=\"Correlation for {}\".format(nmst)\n",
        "    '''grpstk.figure(figsize=(14,8))\n",
        "    grpstk.title(ttl_text,fontsize=25,color=\"b\")\n",
        "    sestk.heatmap(sls.corr(),fmt=\"0.2f\",cmap=secl,annot=True).\n",
        "    grpstk.show()'''\n",
        "    corralldata=pandas.DataFrame(sls.corr()['Amount'])\n",
        "    corralldata=corralldata.fillna(corralldata.mean())\n",
        "    feats=corralldata.index.tolist()\n",
        "    corralldata['Features']=feats\n",
        "    corralldata.columns=[\"Coeff\",\"Features\"]\n",
        "    corralldata=corralldata.reset_index(drop=True)\n",
        "    slscorr=corralldata[(corralldata['Coeff']>0.01)|(corralldata['Coeff']>-0.01)].reset_index(drop=True)\n",
        "    print(\"Selected Features Using Correlation: \\n\",*slscorr.Features.tolist(),sep=\"\\n\")\n",
        "    print(\"---------------------------------------------------\")\n",
        "    print(\"     Total Features Selected Using Correlation: {}\".format(len(slscorr.Features.tolist())))\n",
        "    print(\"---------------------------------------------------\")\n",
        "    return slscorr.Features.tolist()\n",
        "CorrSel=Func1(NrmSls, \"Amazon Sales\")"
      ],
      "metadata": {
        "id": "4aXFfjtpVg0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_sls=NrmSls.drop(['Amount'],axis=1)\n",
        "target_sls=NrmSls['Amount']\n",
        "pred_sls1=pred_sls\n",
        "MdRecurr=feature_selection.RFE(estimator=linear_model.LinearRegression(), verbose=1,\n",
        "                               n_features_to_select=int(len(pred_sls.columns)*0.9),step=0.3)\n",
        "MdRecurr.fit(pred_sls, target_sls)\n",
        "print(MdRecurr.ranking_)\n",
        "MdRecurr.feature_names_in_    #[MdRecurr.support_==True]\n",
        "rankdf=pandas.DataFrame({\"Features\":MdRecurr.feature_names_in_,\"Rank\":MdRecurr.ranking_})\n",
        "rankdf1=rankdf[rankdf['Rank']==1]\n",
        "print(\"Selected Features Using RFE: \\n\",*rankdf1.Features,sep=\"\\n\")\n",
        "print(\"--------------------------------------------------------------\")\n",
        "print(\"---------------------------------------------------\")\n",
        "print(\"     Total Features Selected Using RFE: {}\".format(len(rankdf1.Features)))\n",
        "print(\"---------------------------------------------------\")\n",
        "RFESel=rankdf1.Features.tolist()"
      ],
      "metadata": {
        "id": "foes5gLyVdBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Features_Sales=[]\n",
        "for ff in RFESel:\n",
        "    if ff in CorrSel:\n",
        "        Features_Sales.append(ff)\n",
        "\n",
        "print(\"Selected Features Using Hybrid Approach (RFE+Correlation): {}\".format(len(Features_Sales)))\n"
      ],
      "metadata": {
        "id": "i3MssNcLVoQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import pandas, numpy\n",
        "import os\n",
        "from IPython.display import display, HTML\n",
        "import datetime, calendar\n",
        "from plotly import express\n",
        "import plotly.graph_objects as go\n",
        "import matplotlib.pyplot as slspl, seaborn as sestk\n",
        "from sklearn import metrics, decomposition, utils, model_selection, pipeline,preprocessing, feature_selection\n",
        "from sklearn import ensemble, tree, svm, neural_network, linear_model\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "import pickle"
      ],
      "metadata": {
        "id": "TTkcyk9m6rdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NrmSls1=utils.resample(NrmSls,replace = True, n_samples = int(len(NrmSls)*1.5), random_state = 10).reset_index(drop=True)\n",
        "SlsPred=NrmSls1[Features_Sales]\n",
        "SlsPred"
      ],
      "metadata": {
        "id": "uQSpBa8SV0Aw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SlsAmount=NrmSls1['Amount']\n",
        "SlsAmount"
      ],
      "metadata": {
        "id": "-s_1cgWc6HYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Segmentation\n"
      ],
      "metadata": {
        "id": "9X6EXTOG7GyO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def SegmentData(prd,tr):\n",
        "    TrainSlsX,TestSlsX,TrainSlsY,TestSlsY=model_selection.train_test_split(prd,tr, test_size=0.25, random_state=10)\n",
        "    return TrainSlsX,TestSlsX,TrainSlsY,TestSlsY\n",
        "\n",
        "TrainSlsX,TestSlsX,TrainSlsY,TestSlsY=SegmentData(SlsPred,SlsAmount)"
      ],
      "metadata": {
        "id": "qfJeWHzK6He0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function for Sales Prediction"
      ],
      "metadata": {
        "id": "SZQE-Mf67P_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def SalesPredFunc(rgmd, XSLTR, XSLTS, YSLTR, YSLTS, mdl_pres_sls):\n",
        "    mdl_pres_sls=rgmd\n",
        "    mdl_pres_sls.fit(XSLTR, YSLTR)\n",
        "    prdCov=mdl_pres_sls.predict(XSLTS)\n",
        "    prdCov_tr=mdl_pres_sls.predict(XSLTR)\n",
        "    mse=metrics.mean_squared_error(YSLTS,prdCov,squared=True)\n",
        "    r2nf=round(metrics.r2_score(YSLTS,prdCov),4)*100\n",
        "    r2nftr=round(metrics.r2_score(YSLTR,prdCov_tr),4)*100\n",
        "    mapenf=metrics.mean_absolute_percentage_error(YSLTS,prdCov)\n",
        "    return r2nf,r2nftr,mse,mapenf"
      ],
      "metadata": {
        "id": "NQl8ocTH6Hh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assigning Models with Default Settings"
      ],
      "metadata": {
        "id": "YrpSMJdW7WgO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ModelSales=[\n",
        "    tree.DecisionTreeRegressor(),\n",
        "    linear_model.LinearRegression(),\n",
        "    svm.LinearSVR(),\n",
        "    ensemble.RandomForestRegressor(min_weight_fraction_leaf=0.001),\n",
        "    neural_network.MLPRegressor()\n",
        "]\n",
        "ModelNmSales=[\n",
        "   \"Decision Tree\",\n",
        "    \"Linear Regression\",\n",
        "    \"Support Vector Regression\",\n",
        "    \"Random Forest\",\n",
        "    \"MLP Regression\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "CCdu_0FD6HlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sales Prediction with Models with Default Settings"
      ],
      "metadata": {
        "id": "wlKA5FCR7e1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DataSalesPred=[[],[],[],[],[],[]]\n",
        "for i in range(len(ModelSales)):\n",
        "    Strt_T = datetime.datetime.now()\n",
        "    print(\"Executing {}\".format(ModelNmSales[i]))\n",
        "    PrdCv=SalesPredFunc(ModelSales[i], TrainSlsX, TestSlsX, TrainSlsY, TestSlsY, ModelNmSales[i])\n",
        "    DataSalesPred[0].append(PrdCv[0])\n",
        "    DataSalesPred[1].append(PrdCv[1])\n",
        "    DataSalesPred[2].append(PrdCv[2])\n",
        "    DataSalesPred[3].append(PrdCv[3])\n",
        "    EndT = datetime.datetime.now()\n",
        "    DfTime = EndT-Strt_T\n",
        "    TimeSec=DfTime.total_seconds()\n",
        "    DataSalesPred[4].append(round(TimeSec,3))\n",
        "    print(\"Elasped Time: {} Seconds\".format(round(TimeSec,3)))\n",
        "    print(\"______________________________________\")"
      ],
      "metadata": {
        "id": "JJi32CSJ6Hoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PredSalesDF=pandas.DataFrame({\n",
        "    \"Model\":ModelNmSales,\n",
        "    \"R2_Train(Ex-1)\":DataSalesPred[1],\n",
        "    \"R2_Test(Ex-1)\":DataSalesPred[0],\n",
        "    \"MSE_Test(Ex-1)\":DataSalesPred[2],\n",
        "    \"MAPE_Test(Ex-1)\":DataSalesPred[3],\n",
        "    \"Time_Test(Ex-1)\":DataSalesPred[4]\n",
        "})\n",
        "PredSalesDF\n",
        "\n",
        "clnf_all_lst=PredSalesDF.columns.tolist()[2:]\n",
        "for i in clnf_all_lst:\n",
        "    PredSalesDF=PredSalesDF.sort_values(by=i,ascending=True)\n",
        "    slspl.figure(figsize=(6,4))\n",
        "    slspl.title(\"Comparison of {}\".format(i),fontsize=16)\n",
        "    slspl.barh(PredSalesDF['Model'],PredSalesDF[i],color=[\"m\",\"b\",\"g\",\"c\"])\n",
        "    slspl.xlabel(\"Model\",fontsize=13)\n",
        "    slspl.ylabel(\"{}\".format(i),fontsize=13)\n",
        "    for gid, vlgl in enumerate(PredSalesDF[i]):\n",
        "        slspl.text(vlgl, gid, str(vlgl))\n",
        "    slspl.show()\n",
        "PredSalesDF=PredSalesDF.sort_values(by='R2_Test(Ex-1)',ascending=False).reset_index(drop=True)\n",
        "PredSalesDF.to_csv(\"SalesPredictionEx1.csv\")\n",
        "PredSalesDF"
      ],
      "metadata": {
        "id": "gdSFIk4r70CU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Tuning"
      ],
      "metadata": {
        "id": "fyrEmmXj97Qk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ModelSales1=[\n",
        "    tree.DecisionTreeRegressor(),\n",
        "    linear_model.LinearRegression(),\n",
        "    svm.LinearSVR(),\n",
        "    ensemble.RandomForestRegressor(),\n",
        "    neural_network.MLPRegressor()\n",
        "]\n",
        "TunedModelSales=ModelSales1.copy()\n",
        "HypTunes=[\n",
        "    [{\"max_depth\":[50,60,70],\"splitter\":[\"best\", \"random\"]}],\n",
        "    [{\"fit_intercept\":[False,True]}],\n",
        "    [{'tol': [0.01,0.001,0.0001,0.00001],'C':[0.2,0.4,0.6,0.8,1.0],\"max_iter\":[500,1000,1500]}],\n",
        "    [{\"n_estimators\":[50,100],\"max_depth\":[50,60,70]}],\n",
        "    [{'solver':['lbfgs','adam'],\"alpha\":[0.001,0.0001,0.00001],'tol': [0.001,0.0001,0.00001]}]\n",
        "]\n",
        "params_tuned, time_tuned= [], []\n",
        "rgmdls=[\n",
        "    'DTR.sav',\n",
        "    'LR.sav',\n",
        "    'SVR.sav',\n",
        "    'RFR.sav',\n",
        "    'MLPR.sav'\n",
        "]\n",
        "dirfiles=os.listdir(os.getcwd())\n",
        "if 'DTR.sav' in dirfiles and 'LR.sav' in dirfiles and 'SVR.sav' in dirfiles and 'RFR.sav' in dirfiles and 'MLPR.sav' in dirfiles:\n",
        "    print(\"-----------------------------------------------------\")\n",
        "    print(\"   Tuned Models are Loading Now ...\\n   Loading Model Files\")\n",
        "    print(\"-----------------------------------------------------\")\n",
        "    rg1=pickle.load(open(rgmdls[0], 'rb'))\n",
        "    rg2=pickle.load(open(rgmdls[1], 'rb'))\n",
        "    rg3=pickle.load(open(rgmdls[2], 'rb'))\n",
        "    rg4=pickle.load(open(rgmdls[3], 'rb'))\n",
        "    rg5=pickle.load(open(rgmdls[4], 'rb'))\n",
        "    TunedModelSales=[rg1,rg2,rg3,rg4,rg5]\n",
        "    print(*TunedModelSales, sep=\"\\n\")\n",
        "else:\n",
        "    for c in range(len(TunedModelSales)):\n",
        "        print(\"____________________________________________________\")\n",
        "        print(\"Tuning {}\".format(ModelNmSales[c]))\n",
        "        print(\"____________________________________________________\")\n",
        "        t1 = datetime.datetime.now()\n",
        "        SlsGrdSrch = model_selection.GridSearchCV(TunedModelSales[c], HypTunes[c], cv = 5, scoring='r2')\n",
        "        SlsGrdSrch.fit(TrainSlsX, TrainSlsY)\n",
        "        TunedModelSales[c]=SlsGrdSrch.best_estimator_\n",
        "        t2 = datetime.datetime.now()\n",
        "        delta = t2 - t1\n",
        "        ElapsedTime=delta.total_seconds()\n",
        "        print(\"Time Taken for Tuning '{}': {} Seconds\".format(ModelNmSales[c],ElapsedTime))\n",
        "        print(\"____________________________________________________\")\n",
        "        print(\"{} Tuning Complete\".format(ModelNmSales[c]))\n",
        "        print(\"____________________________________________________\")\n",
        "        params_tuned.append(SlsGrdSrch.best_estimator_)\n",
        "        time_tuned.append(ElapsedTime)\n",
        "    for hm in range(len(TunedModelSales)):\n",
        "        pickle.dump(TunedModelSales[hm], open(rgmdls[hm], 'wb'))\n",
        "    TuningData=pandas.DataFrame({\"Classifier\":ModelNmSales,\"Tuned_Parameters\":params_tuned,\"Tuning_Time(Sec)\":time_tuned})\n",
        "    #display(HTML(TuningData.to_html()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SysDQqWC92Qi",
        "outputId": "30c18032-8e40-437d-d193-0ed9e356e6c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "____________________________________________________\n",
            "Tuning Decision Tree\n",
            "____________________________________________________\n",
            "Time Taken for Tuning 'Decision Tree': 35.33365 Seconds\n",
            "____________________________________________________\n",
            "Decision Tree Tuning Complete\n",
            "____________________________________________________\n",
            "____________________________________________________\n",
            "Tuning Linear Regression\n",
            "____________________________________________________\n",
            "Time Taken for Tuning 'Linear Regression': 1.314008 Seconds\n",
            "____________________________________________________\n",
            "Linear Regression Tuning Complete\n",
            "____________________________________________________\n",
            "____________________________________________________\n",
            "Tuning Support Vector Regression\n",
            "____________________________________________________\n",
            "Time Taken for Tuning 'Support Vector Regression': 68.675929 Seconds\n",
            "____________________________________________________\n",
            "Support Vector Regression Tuning Complete\n",
            "____________________________________________________\n",
            "____________________________________________________\n",
            "Tuning Random Forest\n",
            "____________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "avlwnQyJ9nbF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}